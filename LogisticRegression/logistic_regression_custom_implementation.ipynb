{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-pkonX-va1xc"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LogisticRegression, Lasso, Ridge, ElasticNet\n",
        "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix,ConfusionMatrixDisplay\n",
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load\n",
        "def load_data(view=False):\n",
        "  X, y = load_breast_cancer(return_X_y = True)\n",
        "\n",
        "  # Concatenate\n",
        "  y = y.reshape(-1, 1)\n",
        "  data = np.hstack((X, y))\n",
        "  if view:\n",
        "    print(f\"X is of type {type(X)}, y is {type(y)}.\")\n",
        "    print(f\"Some X examples:\")\n",
        "    print(X[:10])\n",
        "    print(f\"Some y examples:\")\n",
        "    print(y[:10])\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "bCn10zdpa6nI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle and Create Split Training/Dev/Test 60/20/20%\n",
        "def shuffle_and_split(data, view=False):\n",
        "  np.random.shuffle(data)\n",
        "  total_sample = len(data)\n",
        "  train = data[:int(total_sample*0.6)]\n",
        "  dev = data[int(total_sample*0.6):int(total_sample*0.8)]\n",
        "  test = data[int(total_sample*0.8):]\n",
        "\n",
        "  if view:\n",
        "    print(\"Verify shuffling is ok:\")\n",
        "    temp = data[:5]\n",
        "    print(\"Initial Data:\")\n",
        "    print(temp)\n",
        "    print()\n",
        "    print(\"Shuffled:\")\n",
        "    np.random.shuffle(temp)\n",
        "    print(temp)\n",
        "\n",
        "  return total_sample, train, dev, test"
      ],
      "metadata": {
        "id": "pDSN3viXa8Fr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for Data Leaks\n",
        "def data_leaking_check(data1, data2):\n",
        "\tdata_leaking = False\n",
        "\tfor d1 in data1:\n",
        "\t\tfor d2 in data2:\n",
        "\t\t\tif (np.array_equal(d1, d2)):\n",
        "\t\t\t\tdata_leaking = True\n",
        "\t\t\t\tprint(\"Find same sample: \")\n",
        "\t\t\t\tprint(d1)\n",
        "\tif (not data_leaking):\n",
        "\t\tprint(\"No Data Leaking.\")"
      ],
      "metadata": {
        "id": "vJW1QW33a9VF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Features and Labels\n",
        "def get_features_and_labels(data):\n",
        "  features = data[:, :-1]\n",
        "  labels = data[:, -1]\n",
        "\n",
        "  # Reshape labels\n",
        "  labels = labels.reshape(-1, 1)\n",
        "\n",
        "  return features, labels"
      ],
      "metadata": {
        "id": "qoQz93eRa-fa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Polynomial Features\n",
        "def generate_polynomial_features(X, degree):\n",
        "    num_samples, num_features = X.shape\n",
        "    X_poly = np.zeros((num_samples, num_features * degree))\n",
        "    for i in range(num_samples):\n",
        "        for j in range(num_features):\n",
        "            for d in range(1, degree + 1):\n",
        "                # Replace value at location with poly\n",
        "                X_poly[i, j * degree + d - 1] = X[i, j] ** d\n",
        "    return X_poly"
      ],
      "metadata": {
        "id": "lE3RiaGha_qb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cost Function\n",
        "def compute_cost(Y_pred, Y_true, length):\n",
        "  m = length\n",
        "  epsilon = 1e-15  # small value to prevent log(0)\n",
        "  J = -1 / m * np.sum(Y_true * np.log(Y_pred + epsilon) + (1 - Y_true) * np.log(1 - Y_pred + epsilon))\n",
        "\n",
        "  return J"
      ],
      "metadata": {
        "id": "JnpZ50Gte_4g"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_logistic_regression(theta, input):\n",
        "  z = np.dot(input, theta)\n",
        "  prediction = 1 / (1 + np.exp(-z))\n",
        "\n",
        "  # DEBUGGING\n",
        "  # print(f'SHAPE OF INPUT: {input.shape}')\n",
        "  # print(f'SHAPE OF THETA: {theta.shape}')\n",
        "  # print(f'SHAPE OF PREDICTIONS: {prediction.shape}')\n",
        "  return prediction"
      ],
      "metadata": {
        "id": "5VAyNZCtjpk_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Descent\n",
        "def update_theta(theta, X, Y_true, Y_pred, learning_rate, length, penalty='l1', alpha=0.1):\n",
        "  m = length\n",
        "  number_features = X.shape[1]\n",
        "\n",
        "  # compute regular gradient\n",
        "  error = Y_pred - Y_true\n",
        "  gradient = (np.dot(X.T, error) / m).T\n",
        "\n",
        "  # update based on penalty\n",
        "  if penalty == 'l1':\n",
        "    regularized_gradient = gradient + alpha * np.sign(theta)\n",
        "  elif penalty == 'l2':\n",
        "    regularized_gradient = gradient + alpha * theta\n",
        "  elif penalty == 'l12':\n",
        "    l1_gradient = alpha * np.sign(theta)\n",
        "    l2_gradient = alpha * theta\n",
        "    regularized_gradient = gradient + l1_gradient + l2_gradient\n",
        "  else:\n",
        "    regularized_gradient = gradient\n",
        "\n",
        "  updated_theta = np.zeros_like(theta)\n",
        "\n",
        "  # ValueError: setting an array element with a sequence ??\n",
        "  regularized_gradient = regularized_gradient.flatten()\n",
        "\n",
        "  # Iterate over each coefficient and update based on error\n",
        "  for j in range(len(theta)):\n",
        "\n",
        "    # DEBUGGING\n",
        "    # print(f'type of learning rate: {type(learning_rate)}')\n",
        "    # print(f'type of theta: {type(theta)}')\n",
        "    # print(f'type of updated_theta: {type(updated_theta)}')\n",
        "    # print(f'tpye of gradient: {type(regularized_gradient)}')\n",
        "    # print(f'shape of gradient: {regularized_gradient.shape}')\n",
        "\n",
        "    updated_theta[j] = theta[j] - learning_rate * regularized_gradient[j]\n",
        "  return updated_theta"
      ],
      "metadata": {
        "id": "3zkF3AWwfBTP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Cost\n",
        "def plot_cost(k, iterations, cost_history, name):\n",
        "  plt.plot(iterations, cost_history)\n",
        "  plt.title(f'{name} set cost Over {k} Iterations')\n",
        "  plt.xlabel('Iteration')\n",
        "  plt.ylabel('Cost')"
      ],
      "metadata": {
        "id": "slr9K-v-n-ca"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_logistic_regression(data, iterations, penalties, reg_params, polynomial_degree):\n",
        "\n",
        "  # Get splits and initialize tracking stuff\n",
        "  total_sample, training_set, dev_set, test_set = shuffle_and_split(data)\n",
        "  k, cost_change, cost, cost_history, iteration_count = 0, 0, float('inf'), [], []\n",
        "  learning_rate = 0.0001\n",
        "  converged = False\n",
        "\n",
        "  # Check Data\n",
        "  data_leaking_check(training_set, dev_set)\n",
        "\n",
        "  # Get features/labels\n",
        "  X_train, y_train = get_features_and_labels(training_set)\n",
        "  # print(f'X_train {X_train}')\n",
        "  # print(f'y_train {y_train}')\n",
        "  X_dev, y_dev = get_features_and_labels(dev_set)\n",
        "  X_test, y_test = get_features_and_labels(test_set)\n",
        "\n",
        "  # Scale features prior to polynomial feature generation\n",
        "  scaler = StandardScaler()\n",
        "  x_train = scaler.fit_transform(X_train)\n",
        "  x_test = scaler.fit_transform(X_test)\n",
        "\n",
        "  # Generate polynomial features\n",
        "  x_train_poly = generate_polynomial_features(x_train, polynomial_degree)\n",
        "  x_test_poly = generate_polynomial_features(x_test, polynomial_degree)\n",
        "\n",
        "  # Initialize theta\n",
        "  theta = np.zeros(x_train_poly.shape[1])\n",
        "\n",
        "  # Train the model\n",
        "  print('Training custom model...')\n",
        "  while not converged and k < iterations:\n",
        "    prediction = perform_logistic_regression(theta, x_train_poly)\n",
        "    previous_cost = cost\n",
        "    cost = compute_cost(prediction, y_train, len(y_train))\n",
        "    if k == 0:\n",
        "      lowest_cost = cost\n",
        "    cost_change = cost - previous_cost\n",
        "    # print(f'cost change: {cost_change}')\n",
        "    if cost_change > 0:\n",
        "      pass\n",
        "      # can change params here to get cost to go down\n",
        "      # but I'm out of time to write the code\n",
        "      # print('Cost going up...')\n",
        "      # print(f'{previous_cost} changed to {cost}')\n",
        "    else:\n",
        "      # print('Cost going down...')\n",
        "      # print(f'{previous_cost} changed to {cost}')\n",
        "      if abs(cost_change) < 0.001 and cost_change != 0:\n",
        "        print('Convergence (hopefully)! Getting results...')\n",
        "        converged = True\n",
        "        break\n",
        "\n",
        "    theta = update_theta(theta, x_train_poly, y_train, prediction, learning_rate, len(y_train))\n",
        "    best_theta = theta if k == 0 else best_theta\n",
        "\n",
        "    # print(f'UPDATED THETA SHAPE: {theta.shape}')\n",
        "\n",
        "    # Track everything\n",
        "    cost_history.append(cost)\n",
        "    if cost < lowest_cost:\n",
        "      lowest_cost = cost\n",
        "      # print(f'theta  prior to swap: {theta}')\n",
        "      best_theta = theta\n",
        "      # print(f'best_theta : {best_theta}')\n",
        "    k += 1\n",
        "    iteration_count.append(k)\n",
        "\n",
        "  print(f'Lowest Cost of {lowest_cost} was achieved for training.')\n",
        "\n",
        "  # DEBUGGING\n",
        "  # plot_cost(k, iteration_count, cost_history, 'Training')\n",
        "  # print('Before test set, what are these values?')\n",
        "  # print(f'best theta shape: {best_theta.shape}')\n",
        "  # print(f'best theta: {best_theta}')\n",
        "  # print()\n",
        "  # print(f'x_test_poly shape: {x_test_poly.shape}')\n",
        "\n",
        "  print('Running test set...')\n",
        "  # print(f'best theta shape: {best_theta.shape}, type {type(best_theta)}, len {len(best_theta)}')\n",
        "  # print(f'x_test shape: {x_test_poly.shape}, type {type(x_test_poly)}, len {len(x_test_poly)}')\n",
        "  test_predictions = perform_logistic_regression(best_theta, x_test_poly).reshape(-1, 1)\n",
        "\n",
        "  # DEBUGGING\n",
        "  # print('test_predictions:')\n",
        "  # print(test_predictions)\n",
        "  # print()\n",
        "  # print('labels:')\n",
        "  # print(y_test)\n",
        "  # print()\n",
        "  # print(f'shape of test_pred {test_predictions.shape}, labels {y_test.shape}')\n",
        "\n",
        "  # continuous variables gave me SO MANY ISSUES\n",
        "  binary_predictions = (test_predictions >= 0.5).astype(int)\n",
        "  accuracy = np.mean(binary_predictions == y_test)\n",
        "  auc = roc_auc_score(y_test, test_predictions)\n",
        "  print(f'Custom model accuracy: {accuracy}\\nCustom model AUC: {auc}')"
      ],
      "metadata": {
        "id": "QNnkNLh6dc5Z"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sklearn_logistic_regression(data, iterations, penalties, reg_params, polynomial_degrees):\n",
        "  X, y = data[0], data[1]\n",
        "\n",
        "  # print(f'shape X {X.shape}')\n",
        "  # print(f'shape y {y.shape}')\n",
        "\n",
        "  # Get splits a standardize\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.6, random_state=42)\n",
        "\n",
        "  # print(f'X_train: {X_train}')\n",
        "  # print(f'y_train: {y_train}')\n",
        "  # print(f'X_test: {X_test}')\n",
        "  # print(f'y_test: {y_test}')\n",
        "\n",
        "  # I'm not sure why this needed to happen here, but for some reason\n",
        "  # it was throwing an error for it being 1D arrays rather than 2D\n",
        "  X_train = X_train[:, np.newaxis] if len(X_train.shape) == 1 else X_train\n",
        "  X_test = X_test[:, np.newaxis] if len(X_test.shape) == 1 else X_test\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  x_train = scaler.fit_transform(X_train)\n",
        "  x_test = scaler.fit_transform(X_test)\n",
        "\n",
        "  # No continuous variables ? why is this different from custom?\n",
        "  threshold = 0.5\n",
        "  y_train_binary = (y_train > threshold).astype(int)\n",
        "  y_test_binary = (y_test > threshold).astype(int)\n",
        "\n",
        "  # DEBUGGING\n",
        "  # print(f'type y_train: {type(y_train)}')\n",
        "  # print(f'np.unique(y_train): {np.unique(y_train)}')\n",
        "  # print(f'type y_test: {type(y_test)}')\n",
        "  # print(f'np.unique(y_test): {np.unique(y_test)}')\n",
        "\n",
        "  # Get model\n",
        "  model = LogisticRegression()\n",
        "\n",
        "  # Fit model\n",
        "  model.fit(x_train, y_train_binary)\n",
        "\n",
        "  # Predict test values\n",
        "  predictions = model.predict(x_test)\n",
        "  binary_predictions = (predictions > threshold).astype(int)\n",
        "\n",
        "  # Evaluate\n",
        "  accuracy = accuracy_score(y_test_binary, binary_predictions)\n",
        "  auc = roc_auc_score(y_test_binary, binary_predictions)\n",
        "  # classification_report = classification_report(y_test, predictions)\n",
        "\n",
        "  print(f'sklearn model accuracy: {accuracy}\\nauc: {auc}')\n",
        "\n"
      ],
      "metadata": {
        "id": "KB5Ek9LKwXjI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_logistic_regression(custom_implementation, data):\n",
        "  # Parameters\n",
        "  iterations = 1000\n",
        "  penalties = [None, 'L1', 'L2', 'L12']\n",
        "  reg_params = [random.uniform(0, 2) for _ in range(5)]\n",
        "  # polynomial degrees greater than 4 caused me to run out of RAM and crash colab\n",
        "  polynomial_degrees = 3\n",
        "\n",
        "  if custom_implementation:\n",
        "    print('Beginning custom regression implementation...')\n",
        "    # Why was I sending reg_params? For gradient descent? In any case, not used currently\n",
        "    custom_logistic_regression(data, iterations, penalties, reg_params, polynomial_degrees)\n",
        "\n",
        "  else: # use sklearn\n",
        "    print('Beginning sklearn implementation...')\n",
        "    sklearn_logistic_regression(data, iterations, penalties, reg_params, polynomial_degrees)"
      ],
      "metadata": {
        "id": "FoWlt2t8bBop"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loaded into its own cell for consistancy across iterations while developing\n",
        "data = load_data()"
      ],
      "metadata": {
        "id": "rM7UZtuwijDa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run():\n",
        "  try:\n",
        "    # set True/False for custom_implementation\n",
        "    get_logistic_regression(False, data)\n",
        "  except ValueError as e:\n",
        "    print(f'An error occurred: {e}')\n",
        "\n",
        "run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xG2WhghwY7zj",
        "outputId": "11496a59-b4b4-40f4-f036-f7003c25b26c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beginning sklearn implementation...\n",
            "sklearn model accuracy: 0.14285714285714285\n",
            "auc: 0.5\n"
          ]
        }
      ]
    }
  ]
}